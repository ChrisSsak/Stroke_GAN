# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dJAiG1Xewa-RZKYTYBhuOdxK2-jX8Qmb
"""

import numpy as np
import cv2
!pip install -U tensorflow-addons
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
!ls "/content/drive/My Drive/paper"
# %cd "/content/drive/My Drive/paper"

!pip install import-ipynb
import import_ipynb
import Discriminator,loss_functions,image_gen,Generator

import datetime
import time
from IPython import display
import random

log_dir="logs/"

summary_writer = tf.summary.create_file_writer(
  log_dir + "fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))

!pip install wandb
import wandb
wandb.init(project="Stroke_GAN")

import os
import sys
generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)


checkpoint_dir = '/content/drive/MyDrive/Yr3Project/fourstroke_2e-4_noise' 
checkpoint_prefix = os.path.join(checkpoint_dir, "tm")
checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,
                                 discriminator_optimizer=discriminator_optimizer,
                                 generator=gen,
                                 discriminator=disc)


def train_step(real_x, real_y,epoch=1):
  #generate image
 with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
  real_x=real_x[None]
  real_y=real_y[None]
  params=gen(real_x,training=True) # generate params
  gen_tape.watch(params)
  
  fake_image=generate_image_two_col(model,real_x,params[0][:10][None],params[0][-3],params[0][-2],params[0][-1]) #for colour
  #fake_image=generate_image_two_col(model,real_x,params)

  
  disc_real_y = disc([real_x,real_y],training=True)#input image, target image
  disc_fake_y = disc([real_x,fake_image ],training=True)#input image, generated image
  

  # calculate the loss
  #gen_total_loss = generator_loss(fake_image,real_y)#disc output on generated img,  generated image, target'''
  gen_total_loss,gen_gan_loss,gen_l1_loss=generator_loss(disc_fake_y,fake_image,real_y)
  disc_loss = discriminator_loss(disc_real_y, disc_fake_y)#generated img, target
  
  
  
  
  
  generator_gradients = gen_tape.gradient(gen_total_loss,
                                          gen.trainable_variables)
  discriminator_gradients = disc_tape.gradient(disc_loss,
                                               disc.trainable_variables)
 
 #apply gradients
  generator_optimizer.apply_gradients(zip(generator_gradients,
                                          gen.trainable_variables))
  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,
                                              disc.trainable_variables))
 
  with summary_writer.as_default():
    tf.summary.scalar('gen_total_loss', gen_total_loss, step=epoch)
    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=epoch)
    tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=epoch)
    tf.summary.scalar('disc_loss', disc_loss, step=epoch)

  wandb.log({'epoch': epoch, 'gen_total_loss': gen_total_loss,
             'gen_gan_loss':gen_gan_loss,
             'gen_l1_loss': gen_l1_loss,
             'disc_loss':disc_loss})

from IPython.display import clear_output
def fit(epochs):
 for epoch in range(epochs):
  start = time.time()

  n = 0
  for image_x, image_y in zip(trainingimagesy,trainingimagesx):
    train_step(image_x, image_y,epoch)
    if n % 10 == 0:
      print ('.', end='')
    n+=1

  clear_output(wait=True)
  
  checkpoint.save(file_prefix = checkpoint_prefix)
      
  print ('Time taken for epoch {} is {} sec\n'.format(epoch + 1,
                                                        time.time()-start))
 checkpoint.save(file_prefix = checkpoint_prefix)